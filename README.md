<p align="center">
  <img src="unsloth_mlx_logo_f.png" alt="Unsloth-MLX Logo" width="200"/>
</p>
<h1 align="center">Unsloth-MLX</h1>

<p align="center">
  <strong>Fine-tune LLMs on your Mac with Apple Silicon</strong><br>
  <em>Prototype locally, scale to cloud. Same code, just change the import.</em>
</p>

<p align="center">
  <a href="#installation"><img src="https://img.shields.io/badge/Platform-Apple%20Silicon-black?logo=apple" alt="Platform"></a>
  <a href="#requirements"><img src="https://img.shields.io/badge/Python-3.9+-blue?logo=python&logoColor=white" alt="Python"></a>
  <a href="https://github.com/ml-explore/mlx"><img src="https://img.shields.io/badge/MLX-0.20+-green" alt="MLX"></a>
  <a href="#license"><img src="https://img.shields.io/badge/License-Apache%202.0-orange" alt="License"></a>
</p>

<p align="center">
  <a href="#quick-start">Quick Start</a> ¬∑
  <a href="#supported-training-methods">Training Methods</a> ¬∑
  <a href="#examples">Examples</a> ¬∑
  <a href="#project-status">Status</a>
</p>

---

## Why Unsloth-MLX?

Bringing the [Unsloth](https://github.com/unslothai/unsloth) experience to Mac users via Apple's [MLX](https://github.com/ml-explore/mlx) framework.

- üöÄ **Fine-tune LLMs locally** on your Mac (M1/M2/M3/M4/M5)
- üíæ **Leverage unified memory** (up to 512GB on Mac Studio)
- üîÑ **Same API as Unsloth** - your existing code just works!
- üì¶ **Export anywhere** - HuggingFace format, GGUF for Ollama/llama.cpp

```python
# Unsloth (CUDA)                        # Unsloth-MLX (Apple Silicon)
from unsloth import FastLanguageModel   from unsloth_mlx import FastLanguageModel
from trl import SFTTrainer              from unsloth_mlx import SFTTrainer

# Rest of your code stays exactly the same!
```

## What This Is (and Isn't)

**This is NOT** a replacement for Unsloth or an attempt to compete with it. Unsloth is incredible - it's the gold standard for efficient LLM fine-tuning on CUDA.

**This IS** a bridge for Mac users who want to:
- üß™ **Prototype locally** - Experiment with fine-tuning before committing to cloud GPU costs
- üìö **Learn & iterate** - Develop your training pipeline with fast local feedback loops
- üîÑ **Then scale up** - Move to cloud NVIDIA GPUs + original Unsloth for production training

```
Local Mac (Unsloth-MLX)     ‚Üí     Cloud GPU (Unsloth)
   Prototype & experiment          Full-scale training
   Small datasets                  Large datasets
   Quick iterations                Production runs
```

## Project Status

> üöß **Building in Public** - Core features work, advanced features in progress.

| Feature | Status | Notes |
|---------|--------|-------|
| SFT Training | ‚úÖ Stable | Full LoRA fine-tuning |
| Model Loading | ‚úÖ Stable | Any HuggingFace model |
| Save/Export | ‚úÖ Stable | HF format, GGUF |
| DPO/ORPO/GRPO | ‚ö†Ô∏è Beta | API ready, full loss coming |
| Vision Models | ‚ö†Ô∏è Beta | Via mlx-vlm |
| PyPI Package | üîú Soon | Install from source for now |

## Installation

```bash
# From source (recommended for now)
git clone https://github.com/ARahim3/unsloth-mlx.git
cd unsloth-mlx
pip install -e .

# PyPI coming soon!
# pip install unsloth-mlx
```

## Quick Start

```python
from unsloth_mlx import FastLanguageModel, SFTTrainer, SFTConfig

# Load any HuggingFace model
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="mlx-community/Llama-3.2-3B-Instruct-4bit",
    max_seq_length=2048,
    load_in_4bit=True,
)

# Add LoRA adapters
model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_alpha=16,
)

# Train with SFTTrainer (same API as TRL!)
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    tokenizer=tokenizer,
    args=SFTConfig(
        per_device_train_batch_size=2,
        learning_rate=2e-4,
        max_steps=100,
    ),
)
trainer.train()

# Save (same API as Unsloth!)
model.save_pretrained("lora_model")  # Adapters only
model.save_pretrained_merged("merged", tokenizer)  # Full model
model.save_pretrained_gguf("model", tokenizer, quantization_method="q4_k_m")  # GGUF
```

## Supported Training Methods

| Method | Trainer | Status | Use Case |
|--------|---------|--------|----------|
| **SFT** | `SFTTrainer` | ‚úÖ Stable | Instruction fine-tuning |
| **DPO** | `DPOTrainer` | ‚ö†Ô∏è Beta | Preference learning |
| **ORPO** | `ORPOTrainer` | ‚ö†Ô∏è Beta | Combined SFT + preference |
| **GRPO** | `GRPOTrainer` | ‚ö†Ô∏è Beta | Reasoning (DeepSeek R1 style) |
| **KTO** | `KTOTrainer` | ‚ö†Ô∏è Beta | Kahneman-Tversky optimization |
| **SimPO** | `SimPOTrainer` | ‚ö†Ô∏è Beta | Simple preference optimization |
| **VLM** | `VLMSFTTrainer` | ‚ö†Ô∏è Beta | Vision-Language models |

## Examples

Check [`examples/`](examples/) for working code:
- Basic model loading and inference
- Complete SFT fine-tuning pipeline
- RL training methods (DPO, GRPO, ORPO)

## Requirements

- **Hardware**: Apple Silicon Mac (M1/M2/M3/M4/M5)
- **OS**: macOS 13.0+ (15.0+ recommended for large models)
- **Memory**: 16GB+ unified RAM (32GB+ for 7B+ models)
- **Python**: 3.9+

## Comparison with Unsloth

| Feature | Unsloth (CUDA) | Unsloth-MLX |
|---------|----------------|-------------|
| Platform | NVIDIA GPUs | Apple Silicon |
| Backend | Triton Kernels | MLX Framework |
| Memory | VRAM (limited) | Unified (up to 192GB) |
| API | Original | 100% Compatible |
| Best For | Production training | Local dev, large models |

## Contributing

Contributions welcome! Areas that need help:
- Full RL loss implementations (DPO, GRPO)
- Custom MLX kernels for performance
- Documentation and examples
- Testing on different M-series chips

## License

Apache 2.0 - See [LICENSE](LICENSE) file.

## Acknowledgments

- [Unsloth](https://github.com/unslothai/unsloth) - The original, incredible CUDA library
- [MLX](https://github.com/ml-explore/mlx) - Apple's ML framework
- [MLX-LM](https://github.com/ml-explore/mlx-lm) - LLM utilities for MLX
- [MLX-VLM](https://github.com/Blaizzy/mlx-vlm) - Vision model support

---

<p align="center">
  <strong>Community project, not affiliated with Unsloth AI or Apple.</strong><br>
  ‚≠ê Star this repo if you find it useful!
</p>
